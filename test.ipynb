{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  67 232  39   0   0   0   0   0]\n",
      " [  0   0   0   0  62  81   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 120 180  39   0   0   0   0   0]\n",
      " [  0   0   0   0 126 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   2 153 210  40   0   0   0   0   0]\n",
      " [  0   0   0   0 220 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  27 254 162   0   0   0   0   0   0]\n",
      " [  0   0   0   0 222 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 183 254 125   0   0   0   0   0   0]\n",
      " [  0   0   0  46 245 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 198 254  56   0   0   0   0   0   0]\n",
      " [  0   0   0 120 254 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   23 231 254  29   0   0   0   0   0   0]\n",
      " [  0   0   0 159 254 120   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  163 254 216  16   0   0   0   0   0   0]\n",
      " [  0   0   0 159 254  67   0   0   0   0   0   0   0   0   0  14  86 178\n",
      "  248 254  91   0   0   0   0   0   0   0]\n",
      " [  0   0   0 159 254  85   0   0   0  47  49 116 144 150 241 243 234 179\n",
      "  241 252  40   0   0   0   0   0   0   0]\n",
      " [  0   0   0 150 253 237 207 207 207 253 254 250 240 198 143  91  28   5\n",
      "  233 250   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 119 177 177 177 177 177  98  56   0   0   0   0   0 102\n",
      "  254 220   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254 137   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254  57   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254  57   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  255  94   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254  96   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254 153   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  255 153   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  96\n",
      "  254 153   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rescaling\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    4.61361014e-05   2.76816631e-04   2.76816631e-04   2.76816631e-04\n",
      "    1.93771627e-03   2.09150347e-03   2.69127265e-03   3.99846205e-04\n",
      "    2.55286437e-03   3.92156886e-03   3.79853905e-03   1.95309496e-03\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    4.61361022e-04   5.53633261e-04   1.44559785e-03   2.36832001e-03\n",
      "    2.61437916e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.89081123e-03   3.46020772e-03   2.64513656e-03\n",
      "    3.89081123e-03   3.72164557e-03   2.99884658e-03   9.84236947e-04\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   7.53556320e-04\n",
      "    3.66013078e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.86005384e-03   1.43021916e-03   1.26105349e-03\n",
      "    1.26105349e-03   8.61207256e-04   5.99769352e-04   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.76816631e-04\n",
      "    3.36793554e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.89081123e-03   3.04498267e-03   2.79892352e-03\n",
      "    3.79853905e-03   3.70626687e-03   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    1.23029610e-03   2.39907741e-03   1.64552103e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.15263355e-03   1.69165709e-04   0.00000000e+00\n",
      "    6.61284139e-04   2.36832001e-03   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.15301814e-04   1.53787023e-05   2.36832001e-03\n",
      "    3.89081123e-03   1.38408307e-03   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.13763956e-03\n",
      "    3.89081123e-03   2.92195310e-03   3.07574046e-05   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.69165709e-04\n",
      "    2.92195310e-03   3.89081123e-03   1.07650913e-03   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    5.38254564e-04   3.70626687e-03   3.46020772e-03   2.46059219e-03\n",
      "    1.66089972e-03   1.53787023e-05   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.24567479e-03   3.69088817e-03   3.89081123e-03\n",
      "    3.89081123e-03   1.83006539e-03   3.84467508e-04   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   6.92041533e-04   2.86043831e-03\n",
      "    3.89081123e-03   3.89081123e-03   2.30680523e-03   4.15224931e-04\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.46059237e-04\n",
      "    1.43021916e-03   3.87543254e-03   3.89081123e-03   2.87581701e-03\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   3.82929645e-03   3.89081123e-03   3.82929645e-03\n",
      "    9.84236947e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   7.07420229e-04   1.99923129e-03\n",
      "    2.81430222e-03   3.89081123e-03   3.89081123e-03   3.18339095e-03\n",
      "    3.07574046e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    5.99769352e-04   2.27604783e-03   3.52172251e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.89081123e-03   3.84467514e-03   2.79892352e-03\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   3.69088812e-04   1.75317191e-03\n",
      "    3.39869293e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.09111876e-03   1.19953870e-03   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.53710115e-04   1.01499434e-03   3.27566336e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.89081123e-03   3.89081123e-03   3.04498267e-03\n",
      "    1.24567479e-03   3.07574046e-05   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   2.76816631e-04   2.62975786e-03\n",
      "    3.36793554e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.89081123e-03   2.99884658e-03   1.23029610e-03   1.38408315e-04\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    8.45828559e-04   2.64513656e-03   3.47558642e-03   3.89081123e-03\n",
      "    3.89081123e-03   3.89081123e-03   3.89081123e-03   3.75240296e-03\n",
      "    2.04536738e-03   1.69165709e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    2.09150347e-03   3.89081123e-03   3.89081123e-03   3.89081123e-03\n",
      "    3.26028443e-03   2.07612477e-03   2.02998868e-03   2.46059237e-04\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])\n",
    "print(Y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 16s 326us/step - loss: 0.1374 - acc: 0.9603 - val_loss: 0.1408 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14083, saving model to best.weights\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 16s 336us/step - loss: 0.1345 - acc: 0.9616 - val_loss: 0.1384 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14083 to 0.13835, saving model to best.weights\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 14s 298us/step - loss: 0.1320 - acc: 0.9622 - val_loss: 0.1357 - val_acc: 0.9613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13835 to 0.13573, saving model to best.weights\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 15s 311us/step - loss: 0.1281 - acc: 0.9637 - val_loss: 0.1330 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13573 to 0.13300, saving model to best.weights\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 14s 298us/step - loss: 0.1243 - acc: 0.9647 - val_loss: 0.1313 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13300 to 0.13132, saving model to best.weights\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 0.1212 - acc: 0.9653 - val_loss: 0.1288 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13132 to 0.12877, saving model to best.weights\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 14s 300us/step - loss: 0.1182 - acc: 0.9669 - val_loss: 0.1258 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12877 to 0.12580, saving model to best.weights\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 14s 297us/step - loss: 0.1153 - acc: 0.9663 - val_loss: 0.1235 - val_acc: 0.9645\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12580 to 0.12352, saving model to best.weights\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 14s 298us/step - loss: 0.1128 - acc: 0.9685 - val_loss: 0.1244 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12352\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 15s 306us/step - loss: 0.1096 - acc: 0.9686 - val_loss: 0.1208 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12352 to 0.12085, saving model to best.weights\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 15s 304us/step - loss: 0.1085 - acc: 0.9681 - val_loss: 0.1192 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12085 to 0.11924, saving model to best.weights\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 0.1059 - acc: 0.9703 - val_loss: 0.1177 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11924 to 0.11775, saving model to best.weights\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 0.1035 - acc: 0.9711 - val_loss: 0.1153 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11775 to 0.11531, saving model to best.weights\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 16s 329us/step - loss: 0.1006 - acc: 0.9717 - val_loss: 0.1139 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11531 to 0.11394, saving model to best.weights\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 14s 300us/step - loss: 0.0984 - acc: 0.9715 - val_loss: 0.1135 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11394 to 0.11345, saving model to best.weights\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 17s 357us/step - loss: 0.0965 - acc: 0.9716 - val_loss: 0.1114 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11345 to 0.11141, saving model to best.weights\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 18s 366us/step - loss: 0.0946 - acc: 0.9732 - val_loss: 0.1106 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11141 to 0.11055, saving model to best.weights\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 16s 332us/step - loss: 0.0924 - acc: 0.9733 - val_loss: 0.1088 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11055 to 0.10879, saving model to best.weights\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 15s 316us/step - loss: 0.0909 - acc: 0.9745 - val_loss: 0.1082 - val_acc: 0.9683\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10879 to 0.10820, saving model to best.weights\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 16s 331us/step - loss: 0.0896 - acc: 0.9746 - val_loss: 0.1072 - val_acc: 0.9683\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10820 to 0.10723, saving model to best.weights\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 14s 297us/step - loss: 0.0879 - acc: 0.9750 - val_loss: 0.1063 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10723 to 0.10627, saving model to best.weights\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 14s 300us/step - loss: 0.0856 - acc: 0.9757 - val_loss: 0.1069 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10627\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 14s 298us/step - loss: 0.0850 - acc: 0.9762 - val_loss: 0.1028 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10627 to 0.10278, saving model to best.weights\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 14s 302us/step - loss: 0.0827 - acc: 0.9767 - val_loss: 0.1021 - val_acc: 0.9709\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10278 to 0.10205, saving model to best.weights\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 0.0821 - acc: 0.9774 - val_loss: 0.1021 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10205\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 15s 305us/step - loss: 0.0803 - acc: 0.9772 - val_loss: 0.1009 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10205 to 0.10088, saving model to best.weights\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 15s 306us/step - loss: 0.0787 - acc: 0.9786 - val_loss: 0.0994 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10088 to 0.09937, saving model to best.weights\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 14s 301us/step - loss: 0.0773 - acc: 0.9782 - val_loss: 0.1014 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09937\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 0.0751 - acc: 0.9793 - val_loss: 0.0977 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09937 to 0.09768, saving model to best.weights\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 15s 317us/step - loss: 0.0743 - acc: 0.9789 - val_loss: 0.0981 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09768\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 15s 305us/step - loss: 0.0742 - acc: 0.9794 - val_loss: 0.0964 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09768 to 0.09642, saving model to best.weights\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 0.0725 - acc: 0.9796 - val_loss: 0.0973 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09642\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 14s 301us/step - loss: 0.0712 - acc: 0.9797 - val_loss: 0.0945 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09642 to 0.09452, saving model to best.weights\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 0.0703 - acc: 0.9805 - val_loss: 0.0947 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09452\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 15s 304us/step - loss: 0.0684 - acc: 0.9803 - val_loss: 0.0964 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09452\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 14s 301us/step - loss: 0.0683 - acc: 0.9806 - val_loss: 0.0941 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09452 to 0.09406, saving model to best.weights\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 15s 303us/step - loss: 0.0663 - acc: 0.9817 - val_loss: 0.0942 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09406\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 14s 302us/step - loss: 0.0655 - acc: 0.9820 - val_loss: 0.0924 - val_acc: 0.9740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss improved from 0.09406 to 0.09243, saving model to best.weights\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 14s 296us/step - loss: 0.0646 - acc: 0.9821 - val_loss: 0.0919 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09243 to 0.09186, saving model to best.weights\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 14s 292us/step - loss: 0.0636 - acc: 0.9817 - val_loss: 0.0912 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09186 to 0.09121, saving model to best.weights\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 14s 293us/step - loss: 0.0630 - acc: 0.9819 - val_loss: 0.0905 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09121 to 0.09052, saving model to best.weights\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 14s 295us/step - loss: 0.0617 - acc: 0.9831 - val_loss: 0.0906 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09052\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 14s 300us/step - loss: 0.0615 - acc: 0.9830 - val_loss: 0.0906 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09052\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 14s 295us/step - loss: 0.0601 - acc: 0.9827 - val_loss: 0.0889 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09052 to 0.08891, saving model to best.weights\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 14s 295us/step - loss: 0.0598 - acc: 0.9832 - val_loss: 0.0888 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.08891 to 0.08884, saving model to best.weights\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 14s 297us/step - loss: 0.0586 - acc: 0.9834 - val_loss: 0.0888 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.08884 to 0.08878, saving model to best.weights\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 0.0582 - acc: 0.9836 - val_loss: 0.0880 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.08878 to 0.08797, saving model to best.weights\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 14s 297us/step - loss: 0.0565 - acc: 0.9840 - val_loss: 0.0878 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.08797 to 0.08782, saving model to best.weights\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 14s 297us/step - loss: 0.0567 - acc: 0.9845 - val_loss: 0.0863 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.08782 to 0.08634, saving model to best.weights\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 14s 299us/step - loss: 0.0557 - acc: 0.9843 - val_loss: 0.0866 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.08634\n"
     ]
    }
   ],
   "source": [
    "checkp = ModelCheckpoint(filepath='best.weights', verbose=1, save_best_only=True)\n",
    "model_fit = model.fit(X_train, Y_train, batch_size=128, epochs=50,\n",
    "          validation_split=0.2, callbacks=[checkp],\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('best.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.5600%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
